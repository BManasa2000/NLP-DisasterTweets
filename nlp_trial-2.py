# -*- coding: utf-8 -*-
"""NLP_trial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wmhgqz3rOz76FXekaDDgVVQMlBYR_oIN

PS: Had made changes to LDA2Vec. Did not work.
TF-IDF with LSTM, XGBoost not working. Didn't do for SVM. If I did, then didn't check if it was working.
"""

from google.colab import drive
drive.mount('/content/gdrive')

# GLOVE_VECTORS_FILE = "/content/gdrive/MyDrive/NLP_Project/glove.twitter.27B.50d.txt"
GLOVE_VECTORS_FILE = "/content/gdrive/MyDrive/NLP_Project/glove.twitter.27B.100d.txt"
TRAIN_FILE = "/content/gdrive/MyDrive/NLP_Project/train.csv"

# Using embeddings for MLP
import sklearn
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from tensorflow.keras.models import load_model
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
import pickle
import re

import numpy as np

import pandas as pd
import string
from nltk.tokenize import TweetTokenizer
tknzr = TweetTokenizer()

from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(patience=3)

df = pd.read_csv(TRAIN_FILE)
# df_test = pd.read_csv(TEST_FILE)

#dropping columns that will not be used
df = df.drop(columns = ['id','keyword','location'])

print(df.columns)

"""### **PRE PROCESSING THE TRAINING DATA**"""

df_clean = df
#converting to lower case
df_clean['text'] = df.text.str.lower()
#removing duplicate tweets
df_clean.drop_duplicates(subset='text',inplace=True)
# for internet links starting with http/s and hashtags and user mentions using regular exp
df_clean.text = df_clean.text.apply(lambda x: re.sub(r'(?:\@|\#|https?:\/\/)\S+', '', x))
 # for intenet links starting with www
df_clean.text = df_clean.text.apply(lambda x: re.sub(r"www\.[a-z]?\.?(com)+|[a-z]+\.(com)", '', x))
#removing digits
df_clean.text = df_clean.text.apply(lambda x: re.sub(r'[0-9]', '', x))
#removing punctuations
df_clean.text = df_clean.text.apply(lambda x: re.sub(r'[\(\)\-\:\;\!\.\,\?\\\/\*\&\^\%\$\#\@\"\<\>\+\=\-\_\[\]\{\}\~\`\']', ' ', x))

# tokenizing
df_clean['tokens'] = df_clean['text'].apply(tknzr.tokenize)

df_clean.columns

#shuffling the rows in the data frame
df_clean = df_clean.sample(frac = 1)

"""## **I) WORD2VEC**"""

# import gensim, import word2vec, fit on your tokens

# from gensim.test.utils import common_texts
from gensim.models import Word2Vec
model_word2vec = Word2Vec(sentences=df_clean['tokens'], size=100, window=5, min_count=1, workers=4)
model_word2vec.save("word2vec.model")

#model['razed']

max_features = 30000 # total number of words in vocabulary 
max_len = 100  # maximum input token length
tokenizer = Tokenizer(num_words=max_features, split=' ')
tokenizer.fit_on_texts(df_clean['text'])

X = tokenizer.texts_to_sequences(df_clean['text'])
X = pad_sequences(X, maxlen = 100)
Y = pd.get_dummies(df_clean['target']).values

v_size = len(tokenizer.word_index) + 1
embedding_matrix = np.zeros((v_size, 100))
for word, index in tokenizer.word_index.items():
  if word not in model_word2vec.wv.vocab:
    embedding_vector = np.zeros(100)
  else:
    embedding_vector = model_word2vec[word]
  if embedding_vector is not None:
    embedding_matrix[index] = embedding_vector

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, stratify=Y)

"""#I)A) LDA2Vec"""

# from gensim.test.utils import common_texts
from gensim.models import LdaModel
from gensim.corpora.dictionary import Dictionary

common_text = df_clean['tokens']

# Create a corpus from a list of texts
common_dictionary = Dictionary(df_clean['tokens'])
common_corpus = [common_dictionary.doc2bow(text) for text in common_text]

# Train the model on the corpus.
num_topics = 15
lda = LdaModel(common_corpus, num_topics=num_topics)

# print(common_text)

def LDA_topics(model, num_topics):
    word_dict = {};
    for i in range(num_topics):
        words = model_word2vec.show_topic(i, topn = 10);
        word_dict['Words of Topic ' + '{:02d}'.format(i+1)] = [i[0] for i in words];
    return pd.DataFrame(word_dict)

LDA_topics(lda, num_topics)

from gensim.models import CoherenceModel
coherence_using_lda2vec = CoherenceModel(model=lda, texts=common_text, dictionary=common_dictionary, coherence='c_v')
coherence = coherence_using_lda2vec.get_coherence()
print('Coherence Score: ', round(coherence, 3))

svd_matrix = lda.fit(common_corpus)

print(type(coherence_using_lda2vec))

X_lr = 
Y_lr = df_clean['target']
X_lr_train, X_lr_test, Y_lr_train, Y_lr_test = train_test_split(X_lr, Y_lr, test_size = 0.2, random_state = 1)
clf = LogisticRegression(random_state=0).fit(X_lr_train, Y_lr_train)

"""# I)B) MLP WITH WORD2VEC EMBEDDINGS"""

maxlen = 100
model_word2vec_mlp = tf.keras.Sequential([
         tf.keras.layers.Embedding(v_size, 100, weights=[embedding_matrix],input_length = maxlen),
         tf.keras.layers.Flatten(),
         tf.keras.layers.Dense(500,activation='relu'),
         tf.keras.layers.Dense(2,activation='softmax')                  
        ])
model_word2vec_mlp.compile(loss = "binary_crossentropy",optimizer = 'adam',metrics = ['accuracy'])
model_word2vec_mlp.summary()

model_word2vec_mlp.fit(X_train, Y_train, epochs = 3, verbose = 1, validation_split=0.1,  callbacks=[early_stopping])

model_word2vec_mlp.evaluate(X_test, Y_test)

y_pred = model_word2vec_mlp.predict(X_test)
y_pred_single = [np.argmax(i) for i in y_pred]
y_test_single = [np.argmax(i) for i in Y_test]
print(sklearn.metrics.classification_report(y_test_single, y_pred_single))

"""# I)C) LSTM with Word2Vec embeddings"""

maxlen = 100
model_word2vec_lstm = tf.keras.Sequential([
         tf.keras.layers.Embedding(v_size, 100, weights=[embedding_matrix],input_length = maxlen),
         tf.keras.layers.LSTM(128),
         tf.keras.layers.Dense(2,activation='softmax')                  
        ])
model_word2vec_lstm.compile(loss = "binary_crossentropy",optimizer = 'adam',metrics = ['accuracy'])
model_word2vec_lstm.summary()

model_word2vec_lstm.fit(X_train, Y_train, epochs = 3, verbose = 1, validation_split=0.1, callbacks=[early_stopping])

model_word2vec_lstm.evaluate(X_test, Y_test)

y_pred = model_word2vec_lstm.predict(X_test)
y_pred_single = [np.argmax(i) for i in y_pred]
y_test_single = [np.argmax(i) for i in Y_test]
print(sklearn.metrics.classification_report(y_test_single, y_pred_single))

"""##I)D) XGBOOST with WORD2VEC Embeddings"""

xgb_input = []
for i in range(len(df_clean['tokens'])):
  temp = np.zeros(100, dtype=float)
  for j in range(len(df_clean.iloc[i]['tokens'])):
    temp += np.array(model_word2vec[df_clean.iloc[i]['tokens'][j]])
  temp /= len(df_clean.iloc[i]['tokens'])
  xgb_input.append(temp)

xgb_input = np.array(xgb_input)
print(xgb_input.shape)

X_train, X_test, Y_train, Y_test = train_test_split(xgb_input, df_clean['target'], stratify=df_clean['target'],test_size=0.2, random_state=0)

cross_val_score(XGBClassifier(), xgb_input, df_clean['target'])

cross_val_score(XGBClassifier(), xgb_input, df_clean['target']).mean()

"""##I)E) SVM with WORD2VEC Embeddings"""

xgb_input = []
for i in range(len(df_clean['tokens'])):
  temp = np.zeros(100, dtype=float)
  for j in range(len(df_clean.iloc[i]['tokens'])):
    temp += np.array(model_word2vec[df_clean.iloc[i]['tokens'][j]])
  temp /= len(df_clean.iloc[i]['tokens'])
  xgb_input.append(temp)

xgb_input = np.array(xgb_input)

X_train, X_test, Y_train, Y_test = train_test_split(xgb_input, df_clean['target'], stratify=df_clean['target'],test_size=0.2, random_state=0)

# Pipeline(steps=[('standardscaler', StandardScaler()),
#                 ('svc', SVC(gamma='auto'))])
clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf.fit(X_train, Y_train)

y_pred = clf.predict(X_test)

print(sklearn.metrics.accuracy_score(Y_test, y_pred))

print(sklearn.metrics.classification_report(Y_test, y_pred))

"""## **II) FastText**"""

from gensim.models import FastText

model_fast = FastText(size=100, window=3, min_count=1, sentences=df_clean['tokens'], workers=10)
model_fast.save("fasttext.model")

# model_fast.wv.most_similar("accident")

max_features = 30000 # total number of words in vocabulary 
max_len = 100  # maximum input token length
tokenizer = Tokenizer(num_words=max_features, split=' ')
tokenizer.fit_on_texts(df_clean['text'])

X = tokenizer.texts_to_sequences(df_clean['text'])
X = pad_sequences(X, maxlen = 100)
Y = pd.get_dummies(df_clean['target']).values

v_size = len(tokenizer.word_index) + 1
embedding_matrix = np.zeros((v_size, 100))
for word, index in tokenizer.word_index.items():
  if word not in model_fast.wv.vocab:
    embedding_vector = np.zeros(100)
  else:
    embedding_vector = model_fast[word]
  if embedding_vector is not None:
    embedding_matrix[index] = embedding_vector

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, stratify=Y)

"""## II)A) MLP with FastText embeddings"""

maxlen = 100
model_fast_mlp = tf.keras.Sequential([
         tf.keras.layers.Embedding(v_size, 100, weights=[embedding_matrix],input_length = maxlen),
         tf.keras.layers.Flatten(),
         tf.keras.layers.Dense(500,activation='relu'),
         tf.keras.layers.Dense(2,activation='softmax')                  
        ])
model_fast_mlp.compile(loss = "binary_crossentropy",optimizer = 'adam',metrics = ['accuracy'])
model_fast_mlp.summary()

model_fast_mlp.fit(X_train, Y_train, epochs = 3, verbose = 1, validation_split=0.1, callbacks=[early_stopping])

model_fast_mlp.evaluate(X_test, Y_test)

y_pred = model_fast_mlp.predict(X_test)
y_pred_single = [np.argmax(i) for i in y_pred]
y_test_single = [np.argmax(i) for i in Y_test]
print(sklearn.metrics.classification_report(y_test_single, y_pred_single))

"""## II)B) LSTM with FastText embeddings"""

maxlen = 100
model_fast_lstm = tf.keras.Sequential([
         tf.keras.layers.Embedding(v_size, 100, weights=[embedding_matrix],input_length = maxlen),
         tf.keras.layers.LSTM(128),
         tf.keras.layers.Dense(2,activation='softmax')                  
        ])
model_fast_lstm.compile(loss = "binary_crossentropy",optimizer = 'adam',metrics = ['accuracy'])
model_fast_lstm.summary()

model_fast_lstm.fit(X_train, Y_train, epochs = 3, verbose = 1, validation_split=0.1, callbacks=[early_stopping])

model_fast_lstm.evaluate(X_test, Y_test)

y_pred = model_fast_lstm.predict(X_test)
y_pred_single = [np.argmax(i) for i in y_pred]
y_test_single = [np.argmax(i) for i in Y_test]
print(sklearn.metrics.classification_report(y_test_single, y_pred_single))

# y_pred[:3]

sklearn.metrics.accuracy_score(y_test_single, y_pred_single)

"""##II)C) XGBOOST with FastText Embeddings"""

xgb_input = []
for i in range(len(df_clean['tokens'])):
  temp = np.zeros(100, dtype=float)
  for j in range(len(df_clean.iloc[i]['tokens'])):
    temp += np.array(model_fast[df_clean.iloc[i]['tokens'][j]])
  temp /= len(df_clean.iloc[i]['tokens'])
  xgb_input.append(temp)

xgb_input = np.array(xgb_input)
# print(xgb_input.shape)

X_train, X_test, Y_train, Y_test = train_test_split(xgb_input, df_clean['target'], stratify=df_clean['target'],test_size=0.2, random_state=0)

cross_val_score(XGBClassifier(), xgb_input, df_clean['target'])

"""##II)D) SVM with FastText Embeddings

"""

xgb_input = []
for i in range(len(df_clean['tokens'])):
  temp = np.zeros(100, dtype=float)
  for j in range(len(df_clean.iloc[i]['tokens'])):
    temp += np.array(model_fast[df_clean.iloc[i]['tokens'][j]])
  temp /= len(df_clean.iloc[i]['tokens'])
  xgb_input.append(temp)

xgb_input = np.array(xgb_input)
# print(xgb_input.shape)

X_train, X_test, Y_train, Y_test = train_test_split(xgb_input, df_clean['target'], stratify=df_clean['target'],test_size=0.2, random_state=0)

# Pipeline(steps=[('standardscaler', StandardScaler()),
#                 ('svc', SVC(gamma='auto'))])
clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf.fit(X_train, Y_train)

y_pred = clf.predict(X_test)

print(sklearn.metrics.accuracy_score(Y_test, y_pred))

print(sklearn.metrics.classification_report(Y_test, y_pred))

"""## **III) GLoVE**"""

word_to_vec_map = {}
with open(GLOVE_VECTORS_FILE, 'r', encoding='UTF-8') as f:
    words = set()
    for line in f:
      w_line = line.split()
      curr_word = w_line[0]
      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)

# print(word_to_vec_map['the'])

max_features = 30000 # total number of words in vocabulary 
max_len = 100  # maximum input token length
tokenizer = Tokenizer(num_words=max_features, split=' ')
tokenizer.fit_on_texts(df_clean['text'])

X = tokenizer.texts_to_sequences(df_clean['text'])
X = pad_sequences(X, maxlen = 100)
Y = pd.get_dummies(df_clean['target']).values

v_size = len(tokenizer.word_index) + 1
embed_vector_len = word_to_vec_map['the'].shape[0]
embedding_matrix = np.zeros((v_size, 100))

for word, index in tokenizer.word_index.items():
  embedding_vector = word_to_vec_map.get(word)
  if embedding_vector is not None:
    embedding_matrix[index, :] = embedding_vector

# word_to_vec_map.get('the').shape

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, stratify=Y)

"""## III)A) MLP WITH GLOVE WORD EMBEDDINGS"""

maxlen = 100
model_glove_mlp = tf.keras.Sequential([
         tf.keras.layers.Embedding(v_size, 100, weights=[embedding_matrix],input_length = maxlen),
         tf.keras.layers.Flatten(),
         tf.keras.layers.Dense(500,activation='relu'),
         tf.keras.layers.Dense(2,activation='softmax')                  
        ])
model_glove_mlp.compile(loss = "binary_crossentropy",optimizer = 'adam',metrics = ['accuracy'])
model_glove_mlp.summary()

model_glove_mlp.fit(X_train, Y_train, epochs = 3, verbose = 1, validation_split=0.1, callbacks=[early_stopping])

model_glove_mlp.evaluate(X_test, Y_test)

y_pred = model_glove_mlp.predict(X_test)
y_pred_single = [np.argmax(i) for i in y_pred]
y_test_single = [np.argmax(i) for i in Y_test]
print(sklearn.metrics.classification_report(y_test_single, y_pred_single))

"""## III)B) LSTM WITH GLOVE WORD EMBEDDINGS"""

maxlen = 100
model_glove_lstm = tf.keras.Sequential([
         tf.keras.layers.Embedding(v_size, 100, weights=[embedding_matrix],input_length = maxlen),
         tf.keras.layers.LSTM(128),
         tf.keras.layers.Dense(2,activation='softmax')                  
        ])
model_glove_lstm.compile(loss = "binary_crossentropy",optimizer = 'adam',metrics = ['accuracy'])
model_glove_lstm.summary()

model_glove_lstm.fit(X_train, Y_train, epochs = 3, verbose = 1, validation_split=0.1, callbacks=[early_stopping])

model_glove_lstm.evaluate(X_test, Y_test)

y_pred = model_glove_lstm.predict(X_test)
y_pred_single = [np.argmax(i) for i in y_pred]
y_test_single = [np.argmax(i) for i in Y_test]
print(sklearn.metrics.classification_report(y_test_single, y_pred_single))

"""##III)C) XGBOOST with GLOVE Embeddings"""

xgb_input = []
for i in range(len(df_clean['tokens'])):
  temp = np.zeros(100, dtype=float)
  for j in range(len(df_clean.iloc[i]['tokens'])):
    embedding_vector = word_to_vec_map.get(df_clean.iloc[i]['tokens'][j])
    if embedding_vector is not None:
      temp += embedding_vector
  temp /= len(df_clean.iloc[i]['tokens'])
  xgb_input.append(temp)

xgb_input = np.array(xgb_input)

X_train, X_test, Y_train, Y_test = train_test_split(xgb_input, df_clean['target'], stratify=df_clean['target'],test_size=0.2, random_state=0)

cross_val_score(XGBClassifier(), xgb_input, df_clean['target'])

"""##III)E) SVM with GLoVE Embeddings"""

xgb_input = []
for i in range(len(df_clean['tokens'])):
  temp = np.zeros(100, dtype=float)
  for j in range(len(df_clean.iloc[i]['tokens'])):
    embedding_vector = word_to_vec_map.get(df_clean.iloc[i]['tokens'][j])
    if embedding_vector is not None:
      temp += embedding_vector
  temp /= len(df_clean.iloc[i]['tokens'])
  xgb_input.append(temp)

xgb_input = np.array(xgb_input)

X_train, X_test, Y_train, Y_test = train_test_split(xgb_input, df_clean['target'], stratify=df_clean['target'],test_size=0.2, random_state=0)

# Pipeline(steps=[('standardscaler', StandardScaler()),
#                 ('svc', SVC(gamma='auto'))])
clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf.fit(X_train, Y_train)

y_pred = clf.predict(X_test)

print(sklearn.metrics.accuracy_score(Y_test, y_pred))

print(sklearn.metrics.classification_report(Y_test, y_pred))

"""##**IV) TF-IDF**"""

df_clean_tfidf = df_clean

# removing stop words

stopwords = [ "a", "about", "above", "after", "again", "against", "all", "am", "an", "and", "any", "are", "as", "at", "be", "because", 
             "been", "before", "being", "below", "between", "both", "but", "by", "could", "did", "do", "does", "doing", "down", "during",
             "each", "few", "for", "from", "further", "had", "has", "have", "having", "he", "he'd", "he'll", "he's", "her", "here", 
             "here's", "hers", "herself", "him", "himself", "his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "if", "in", "into",
             "is", "it", "it's", "its", "itself", "let's", "me", "more", "most", "my", "myself", "nor", "of", "on", "once", "only", "or",
             "other", "ought", "our", "ours", "ourselves", "out", "over", "own", "same", "she", "she'd", "she'll", "she's", "should", 
             "so", "some", "such", "than", "that", "that's", "the", "their", "theirs", "them", "themselves", "then", "there", "there's",
             "these", "they", "they'd", "they'll", "they're", "they've", "this", "those", "through", "to", "too", "under", "until", "up",
             "very", "was", "we", "we'd", "we'll", "we're", "we've", "were", "what", "what's", "when", "when's", "where", "where's",
             "which", "while", "who", "who's", "whom", "why", "why's", "with", "would", "you", "you'd", "you'll", "you're", "you've",
             "your", "yours", "yourself", "yourselves" ]

df_clean_tfidf['text'] = df_clean_tfidf['text'].apply(lambda x : ' '.join([word for word in x.split() if word not in (stopwords)]))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation

v = TfidfVectorizer()
x = v.fit_transform(df_clean_tfidf['text'])
x = x.toarray()

Y = pd.get_dummies(df_clean['target']).values

X_train, X_test, Y_train, Y_test = train_test_split(x, Y, test_size = 0.1, random_state = 1)

vocab = v.get_feature_names_out()
print(vocab[-1])
print(vocab.shape)

# print(X[10])
c = 0
for i in x:
  for j in i:
    if j == 1:
      c += 1

print(c)

"""## IV)A) MLP WITH TF-IDF VECTORS"""

maxlen = 100

model_tfidf_mlp = tf.keras.Sequential()
# model_tfidf_mlp.add(tf.keras.layers.Flatten())
model_tfidf_mlp.add(tf.keras.layers.Dense(500, activation='relu'))
model_tfidf_mlp.add(tf.keras.layers.Dense(2, activation='softmax'))
model_tfidf_mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model_tfidf_mlp.build(X_train.shape)
model_tfidf_mlp.summary()

model_tfidf_mlp.fit(X_train, Y_train, epochs = 5, verbose = 1, validation_split=0.1, callbacks=[early_stopping])

model_tfidf_mlp.evaluate(X_test, Y_test)

y_pred = model_tfidf_mlp.predict(X_test)
y_pred_single = [np.argmax(i) for i in y_pred]
y_test_single = [np.argmax(i) for i in Y_test]
print(sklearn.metrics.classification_report(y_test_single, y_pred_single))

"""## IV)B) LSTM WITH TF-IDF VECTORS"""

X_train_lstm = X_train[:, :, None]
print(X_train_lstm.shape)

maxlen = 100

model_tfidf_lstm = tf.keras.Sequential()
# model_tfidf_lstm.add(tf.keras.layers.LSTM(100, input_shape=(None, data_dim),return_sequences=True))
model_tfidf_lstm.add(tf.keras.layers.LSTM(units=6, input_shape = X_train_lstm.shape, return_sequences = True))
model_tfidf_lstm.add(tf.keras.layers.LSTM(128))
model_tfidf_lstm.add(tf.keras.layers.Dense(2,activation='softmax'))
model_tfidf_lstm.compile(loss = "binary_crossentropy", optimizer = 'adam', metrics = ['accuracy'])
model_tfidf_lstm.summary()

# model = Sequential()
# model.add(LSTM(100, input_shape=(None, data_dim),return_sequences=True))
# model.add(Dropout(0.2))
# model.add(LSTM(200))
# model.add(Dropout(0.2))
# model.add(Dense(1, activation='sigmoid'))
# model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])

model_tfidf_lstm.fit(X_train, Y_train, epochs = 3, verbose = 1, validation_split=0.1, callbacks=[early_stopping])

model_tfidf_lstm.evaluate(X_test, Y_test)

y_pred = model_tfidf_lstm.predict(X_test)
y_pred_single = [np.argmax(i) for i in y_pred]
y_test_single = [np.argmax(i) for i in Y_test]
print(sklearn.metrics.classification_report(y_test_single, y_pred_single))

"""## IV)C) XGBoost with TF-IDF Vectors"""

# xgb_input = []
# for i in range(len(df_clean['tokens'])):
#   temp = np.zeros(100, dtype=float)
#   for j in range(len(df_clean.iloc[i]['tokens'])):
#     temp += np.array(model_fast[df_clean.iloc[i]['tokens'][j]])
#   temp /= len(df_clean.iloc[i]['tokens'])
#   xgb_input.append(temp)

xgb_input = x
xgb_input = np.array(xgb_input)

X_train, X_test, Y_train, Y_test = train_test_split(xgb_input, df_clean_tfidf['target'], stratify=df_clean_tfidf['target'],test_size=0.2, random_state=0)

print(xgb_input)

cross_val_score(XGBClassifier(), xgb_input, df_clean_tfidf['target'])

"""##**V) LSA**"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import Pipeline
documents = df_clean['text']
  
# raw documents to tf-idf matrix: 
v = TfidfVectorizer()
# x = v.fit_transform(df_clean_tfidf['text'])

# SVD to reduce dimensionality: 
svd_model = TruncatedSVD(n_components=2,
                         algorithm='randomized',
                         n_iter=10)
# pipeline of tf-idf + SVD, fit to and applied to documents:
svd_transformer = Pipeline([('tfidf', v), 
                            ('svd', svd_model)])
svd_matrix = svd_transformer.fit_transform(documents)

print(svd_matrix.shape)
# svd_matrix can later be used to compare documents, compare words, or compare queries with documents

topic_encoded_df = pd.DataFrame(svd_matrix, columns=["Topic1", "Topic2"])
# topic_encoded_df["text"] = df_clean['text']

# print(topic_encoded_df[["text", "Topic1", "Topic2"]][:5])

print(topic_encoded_df.shape)

# dictionary = vectorizer.get_feature_names()
# print(dictionary)

# encoding_matrix = pd.DataFrame(svd_model.components_, index=['Topic1', 'Topic2'], columns=dictionary).T
# print(encoding_matrix)

# encoding_matrix['Topic1'] = np.abs(encoding_matrix['Topic1']) 
# encoding_matrix['Topic2'] = np.abs(encoding_matrix['Topic2'])

# encoding_matrix.sort_values('Topic1', ascending=True)
# encoding_matrix.sort_values('Topic2', ascending=True)

# print(encoding_matrix)

X_lr = svd_matrix
Y_lr = df_clean['target']
X_lr_train, X_lr_test, Y_lr_train, Y_lr_test = train_test_split(X_lr, Y_lr, test_size = 0.2, random_state = 1)
clf = LogisticRegression(random_state=0).fit(X_lr_train, Y_lr_train)

clf.predict(X_lr)

y_pred = clf.predict(X_lr_test)
print(sklearn.metrics.classification_report(Y_lr_test, y_pred))

"""##**VII) LDA**"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation

v = TfidfVectorizer()
x = v.fit_transform(df_clean_tfidf['text'])

lda_model = LatentDirichletAllocation(n_components = 2, max_iter = 20, random_state = 20)
# fit transform on model on our count vectorizer : running this will return our topics
X_topics = lda_model.fit_transform(x)

topic_words = lda_model.components_

print(topic_words)
print(len(X_topics))

X_topics

X_lr = X_topics
Y_lr = df_clean_tfidf['target']
X_lr_train, X_lr_test, Y_lr_train, Y_lr_test = train_test_split(X_lr, Y_lr, test_size = 0.2, random_state = 1)
clf = LogisticRegression(random_state=1).fit(X_lr_train, Y_lr_train)

clf.predict(X_lr)

clf.score(X_lr_test, Y_lr_test)

y_pred = clf.predict(X_lr_test)
print(sklearn.metrics.classification_report(Y_lr_test, y_pred))